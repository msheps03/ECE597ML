{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0pmyE9e2n3Q"
      },
      "source": [
        "## Introduction to Homework\n",
        "\n",
        "Total Marks 100 points\n",
        "\n",
        "Extra Credit 30 points\n",
        "\n",
        "\n",
        "In this homework, you will create machine learning models using K Nearest Neighbor, Decision Tree and Random Forests for Fashion MNIST dataset. Specific instruction for that part of the problem can be found in the corresponding cells above the code. \n",
        "\n",
        "Note, you only know the labels of the training dataset. The labels of the test dataset are hidden from you. You will perfrom model selection with cross-validation on the training set.  \n",
        "\n",
        "After obtaining parameters, use the code given to generate submissions, and upload you submissions to Kaggle. Kaggle score tells you, the performance of your model with respect to the test dataset. You can try and fine tune your parameters to be in the top 20 percent of the submissions for extra credit of 20 points. \n",
        "\n",
        "How to participate in the kaggle Competition:\n",
        "\n",
        "1. Create an account on Kaggle.com\n",
        "2. Use the link https://www.kaggle.com/c/ece597-697/ to join the competition. Please ensure that you only join using one account. At the top of the jupyter notebook, pdf(s) mention the name you have used for submission. \n",
        "3. Replace the classifier \"xgb_clf\" with the corresponding classifier for your submission(KNN or Decision Tree or Random Forest) and generate submission.csv\n",
        "4. Upload the submission.csv to see your score on the leaderboard\n",
        "5. To gain extra credit points, try to score higher on the leaderboard\n",
        "\n",
        "\n",
        "Bonous Tips:\n",
        "\n",
        "1. You can directly run this notebook on Google colab if your machine is slower. Upload the data and get started!\n",
        "\n",
        "2. Go through the documentation of sklearn carefully.\n",
        "\n",
        "3. Don't try to run the code for all 60,000 data points. Rather first try to verify implementation using 10,000 data points, scale it up to 60,000. If you don't do this, you'll spend lot more time debugging between each iteration. Make the code work first. \n",
        "\n",
        "4. Complete all the classifiers before trying to optimize based on the leaderboard.\n",
        "\n",
        "5. For cross-validation, you can use gridsearchcv\n",
        "\n",
        "6. Start early! It will give you more time to improve your kaggle leaderboard\n",
        "\n",
        "Good Luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "kWF3HSdSCEFg",
        "outputId": "64bedfe8-a1e3-4af3-b5ab-2c2f51031fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape:  (60000, 784)\n",
            "y_train shape : (60000,)\n",
            "x_test shape:  (10000, 784)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        }
      ],
      "source": [
        "## Code to load data from train and test csv(s)\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train=pd.read_csv(\"train.csv\")\n",
        "test=pd.read_csv(\"test.csv\")\n",
        "\n",
        "X_train = train.iloc[:,2:].to_numpy()\n",
        "y_train = train.iloc[:,:1].to_numpy()\n",
        "\n",
        "m,n = y_train.shape\n",
        "\n",
        "y_train = y_train.reshape(m)\n",
        "\n",
        "X_test = test.iloc[:,1:].to_numpy()\n",
        "\n",
        "def showImage(data):\n",
        "    some_article = data\n",
        "    some_article_image = some_article.reshape(28, 28) # Reshaping it to get the 28x28 pixels\n",
        "    plt.imshow(some_article_image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "print('x_train shape: ', X_train.shape)\n",
        "print('y_train shape :', y_train.shape)\n",
        "\n",
        "print('x_test shape: ', X_test.shape)\n",
        "#print('y_test shape :', y_test.shape)\n",
        "\n",
        "showImage(X_train[1])\n",
        "print(y_train[1])\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTu7-SSWvTCd"
      },
      "source": [
        "##KNN Classifier\n",
        "\n",
        "(20 points)Implement a KNN classifier with 5-fold cross validation. What is the best value of n that you obtained? What happens if you increase value of n more than your best value? Use {3, 5, 7, 9, 11} values for n. \n",
        "\n",
        "No need to submit KNN predictions on Kaggle.\n",
        "\n",
        "(5 points) What is the time complexity of the k-NN algorithm with naive search approach? How can you improve upon the naive search to reduce the time complexity? \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.267666666666667\n",
            "4.27\n",
            "4.2644166666666665\n",
            "4.257583333333333\n",
            "4.24875\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores_list = []\n",
        "n_Values = [3,5,7,9,11]\n",
        "\n",
        "for i in n_Values:\n",
        "    t1 = KNeighborsClassifier(n_neighbors=i)\n",
        "    t1.fit(X_train,y_train)\n",
        "    scores_list.append(cross_val_score(t1,X_train,y_train,cv=5,scoring='accuracy'))\n",
        "\n",
        "for i in scores_list:\n",
        "    print(sum(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ANSWER: Best value of n = 5; Best score = 4.27; The score and accuracy of the model starts reducing when choosing value of n more than 5;\n",
        "The runtime of a naive implementation is pN^2; We can improve this\n",
        "by using recursive algorithms which brings the run time to pN log N."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZOiEFN6dVFu"
      },
      "source": [
        "## Decision Tree Classifier 1\n",
        "\n",
        "(10 points) Train five different decision trees. Use the following max depths (10, 11, 12, 13, 14) How does the maximum depth of the tree affect the estimated accuracy? Explain in at most 4 sentences. Choose the model with lowest estimated out of sample error, train it with the full training set, and predict the labels for the images in the test set using Kagglization code given at the end of the notebook. Upload your predictions to Kaggle and report the\n",
        "accuracy on the public leaderboard by pasting a screenshot in your code.pdf. Is the predicted accuracy close to that of the test set? Make sure that your report clearly states which model was chosen and why.\n",
        "\n",
        "(5 points) What does default value ccp_alpha=0.0 signify for the decision tree classifier?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.8479666666666666, 0.86795, 0.88665, 0.9056833333333333, 0.9266166666666666]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "scores = []\n",
        "for i in range(10,15):\n",
        "    model = tree.DecisionTreeClassifier(max_depth=i)\n",
        "    model.fit(X_train,y_train)\n",
        "    scores.append(model.score(X_train,y_train))\n",
        "\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ANSWER: The estimated accuracy of the decision tree increases with the increase in max_deapth. In the above example, we can see that that max_deapth 14 has the highest accuracy (92.7%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9428166666666666"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "model1 = tree.DecisionTreeClassifier(max_depth=15)\n",
        "model1.fit(X_train,y_train)\n",
        "model1.score(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The estimate accuracy on kaggle is much lower than the test set. Kaggle = 0.806 while test set = 0.993. ##\n",
        "ccp_alpha = 0.0 --> Complexity parameter used for Minimal Cost-Complexity Pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmokAZyRaZPR"
      },
      "source": [
        "## Decision Tree Classifier 2\n",
        "\n",
        "(10 points) Train five different decision trees using five-fold cross validation. Use the following values for max depth (10, 13, 16, 19). Keep all the other parameters to default value. How does the maximum depth of the tree affect the estimated accuracy? Explain in at most 4 sentences. Choose the model with lowest estimated out of sample error, train it with the full training set, and predict the labels for the images in the test set. Finally using Kagglization code given at the end of the notebook generate predictions. Upload your predictions to Kaggle as well as report the position on the public leaderboard by pasting a screenshot in your code.pdf. Is the accuracy obtained on training set, close to that of the test set(kaggle leaderboard)? Make sure that your report clearly states which max depth was chosen and why.\n",
        "\n",
        "(10 points) Compare the best tree obtained for max-depth, with the best tree classifier obtained for ccp_alpha. Is there a difference in their errors? Why?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0102916666666668\n",
            "1.0193333333333334\n",
            "1.0097916666666666\n",
            "1.003875\n"
          ]
        }
      ],
      "source": [
        "max_depth = [10,13,16,19]\n",
        "scores2 = []\n",
        "for i in max_depth:\n",
        "    t2 = tree.DecisionTreeClassifier(max_depth=i)\n",
        "    t2.fit(X_train,y_train)\n",
        "    scores2.append(cross_val_score(t2,X_train,y_train,cv=5,scoring='accuracy'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best value of max depth = 13. The accuracy decreses after max depth = 13."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.90565"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t3 = tree.DecisionTreeClassifier(max_depth=13)\n",
        "t3.fit(X_train,y_train)\n",
        "t3.score(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The accuracy on kaggle is much lower (0.82) vs test set score (0.91). I chose max_depth = 13 because it has the highest accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtSUG0s2g0Fk"
      },
      "source": [
        "## Random Forest Classifier "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rs5L59QdQSB"
      },
      "source": [
        "(20 points) Create a random forest with 150 estimators and using out of bag classification score set to True. \n",
        "\n",
        "Create another random forest with 150 estimators without using out of bag score and bootstrap. Cross validate over 'max_features' with values [10,28,50]. \n",
        "\n",
        "Use the best random forest out of all the forests you created to predict labels in test.csv. Generate predictions using the kagglization code given at the end of the notebook. Upload your predictions obtained to Kaggle and report the accuracy on the public leaderboard by pasting a screenshot in the code.pdf. Is the predicted accuracy close to that of the test set? Make sure that your report clearly states which model was chosen and why?\n",
        "\n",
        "(5 points) Compare the output of both the forests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.88075   , 0.88241667, 0.88216667, 0.88025   , 0.88441667])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "t4 = RandomForestClassifier(n_estimators=150,oob_score=True)\n",
        "t4.fit(X_train,y_train)\n",
        "cross_val_score(t4,X_train,y_train,cv=5,scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.41000001"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum([0.88075   , 0.88241667, 0.88216667, 0.88025   , 0.88441667])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "j:  4.41975\n",
            "j:  4.440083333333334\n",
            "j:  4.446\n"
          ]
        }
      ],
      "source": [
        "max_features = [10,28,50]\n",
        "scores3 = []\n",
        "for i in max_features:\n",
        "    t5 = RandomForestClassifier(n_estimators=150,oob_score=False,bootstrap=False,max_features=i)\n",
        "    t5.fit(X_train,y_train)\n",
        "    scores3.append(cross_val_score(t5,X_train,y_train,cv=5,scoring='accuracy'))\n",
        "\n",
        "for j in scores3:\n",
        "    print('j: ',sum(j))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model with max_features = 50 has the highest accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t6 = RandomForestClassifier(n_estimators=150,oob_score=False,bootstrap=False,max_features=200)\n",
        "t6.fit(X_train,y_train)\n",
        "t6.score(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1tEZfYFMTN"
      },
      "source": [
        "\n",
        "\n",
        "(15 points) Can you visualize the most important Random Forest Classifier features?  (Hint: Obtain feature importances and visualize them by reshaping the data)\n",
        "\n",
        "\n",
        "## Extra credit\n",
        "\n",
        "(10 points) Can you sort and remove features that insignificant, to improve the testing time? Show this using code that removing certain features doesn't drastically change the error, but improves speed of testing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1CQeXO4dbK8"
      },
      "source": [
        "## Kagglization code \n",
        "\n",
        "use this code to generate prediction.csv for you classifier. Upload the predictions to the kaggle competition. Replace xgb_clf by corresponding classifier to obtain your prediction.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "sJyZcVtjU0mt"
      },
      "outputs": [],
      "source": [
        "## code to generate predictions\n",
        "\n",
        "import csv\n",
        "\n",
        "predictions = np.zeros(10000,)\n",
        "for i in range(0,10000):\n",
        "  predictions[i] = int((model1.predict(X_test[i].reshape(1, -1)))) ## make change in this line for each classifier upload output from Decision Tree and random forest on Kaggle!\n",
        "\n",
        "# pd\n",
        "prediction = pd.DataFrame(predictions, columns=['label']).astype(int).to_csv('prediction.csv')\n",
        "\n",
        "format_read=pd.read_csv(\"prediction.csv\")\n",
        "format_read.columns = [\"id\",\"label\"]\n",
        "format_read.to_csv(\"prediction.csv\",index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "homework 4 Problems",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
