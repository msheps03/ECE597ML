{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression for SMS spam classification\n",
    "\n",
    "\n",
    "Each line of the data file `sms.txt`\n",
    "contains a label---either \"spam\" or \"ham\" (i.e. non-spam)---followed\n",
    "by a text message. Here are a few examples (line breaks added for readability):\n",
    "\n",
    "    ham     Ok lar... Joking wif u oni...\n",
    "    ham     Nah I don't think he goes to usf, he lives around here though\n",
    "    spam    Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\n",
    "            Text FA to 87121 to receive entry question(std txt rate)\n",
    "            T&C's apply 08452810075over18's\n",
    "    spam    WINNER!! As a valued network customer you have been\n",
    "            selected to receivea £900 prize reward! To claim\n",
    "            call 09061701461. Claim code KL341. Valid 12 hours only.\n",
    "\n",
    "To create features suitable for logistic regression, code is provided to do the following (using tools from the ``sklearn.feature_extraction.text``):\n",
    "\n",
    "* Convert words to lowercase.\n",
    "* Remove punctuation and special characters (but convert the \\$ and\n",
    "  £ symbols to special tokens and keep them, because these are useful for predicting spam).\n",
    "* Create a dictionary containing the 3000 words that appeared\n",
    "  most frequently in the entire set of messages.\n",
    "* Encode each message as a vector $\\mathbf{x}^{(i)} \\in\n",
    "  \\mathbb{R}^{3000}$. The entry $x^{(i)}_j$ is equal to the\n",
    "  number of times the $j$th word in the dictionary appears in that\n",
    "  message.\n",
    "* Discard some ham messages to have an\n",
    "  equal number of spam and ham messages.\n",
    "* Split data into a training set of 1000 messages and a\n",
    "  test set of 400 messages.\n",
    "  \n",
    "Follow the instructions below to complete the implementation. Your job will be to:\n",
    "\n",
    "* Learn $\\boldsymbol{\\theta}$ by gradient descent\n",
    "* Plot the cost history\n",
    "* Make predictions and report the accuracy on the test set\n",
    "* Test out the classifier on a few of your own text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prep data\n",
    "This cell preps the data. Take a look to see how it works, and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "\n",
    "from logistic_regression import logistic, cost_function, gradient_descent\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Preprocess the SMS Spam Collection data set\n",
    "#  \n",
    "#   https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "# \n",
    "\n",
    "\n",
    "numTrain    = 1000\n",
    "numTest     = 494\n",
    "numFeatures = 3000\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Open the file\n",
    "f = codecs.open('sms.txt', encoding='utf-8')\n",
    "\n",
    "labels = []    # list of labels for each message\n",
    "docs   = []    # list of messages\n",
    "\n",
    "# Go through each line of file and extract the label and the message\n",
    "for line in f:\n",
    "    l, d= line.strip().split('\\t', 1)\n",
    "    labels.append(l)\n",
    "    docs.append(d)\n",
    "\n",
    "# This function will be called on each message to preprocess it\n",
    "def preprocess(doc):\n",
    "    # Replace all currency signs and some url patterns by special\n",
    "    # tokens. These are useful features.\n",
    "    doc = re.sub('[£$]', ' __currency__ ', doc)\n",
    "    doc = re.sub('\\://', ' __url__ ', doc)\n",
    "    doc = doc.lower() # convert to lower\n",
    "    return doc\n",
    "\n",
    "\n",
    "# This is the object that does the conversion from text to feature vectors\n",
    "vectorizer = CountVectorizer(max_features=numFeatures, preprocessor=preprocess)\n",
    "\n",
    "# Do the conversion (\"fit\" the transform from text to feature vector. \n",
    "#   later we will also \"apply\" the tranform on test messages)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Convert labels to numbers: 1 = spam, 0 = ham\n",
    "y = np.array([l == 'spam' for l in labels]).astype('int')\n",
    "\n",
    "# The vectorizer returns sparse scipy arrays. Convert this back to a dense \n",
    "#   numpy array --- not as efficient but easier to work with\n",
    "X = X.toarray()\n",
    "m,n = X.shape\n",
    "\n",
    "# Add a column of ones\n",
    "X = np.column_stack([np.ones(m), X])\n",
    "\n",
    "# \n",
    "# Now massage and split into test/train\n",
    "# \n",
    "pos = np.nonzero(y == 1)[0]   # indices of positive training examples\n",
    "neg = np.nonzero(y == 0)[0]   # indices of negative training examples\n",
    "\n",
    "npos = len(pos)\n",
    "\n",
    "# Create a subset that has the same number of positive and negative examples\n",
    "subset = np.concatenate([pos, neg[0:len(pos)] ])\n",
    "\n",
    "# Randomly shuffle order of examples\n",
    "np.random.shuffle(subset)\n",
    "      \n",
    "X = X[subset,:]\n",
    "y = y[subset]\n",
    "\n",
    "# Split into test and train\n",
    "train = np.arange(numTrain)\n",
    "test  = numTrain + np.arange(numTest)\n",
    "\n",
    "X_train = X[train,:]\n",
    "y_train = y[train]\n",
    "\n",
    "X_test  = X[test,:]\n",
    "y_test  = y[test]\n",
    "\n",
    "# Extract the list of test documents\n",
    "test_docs = [docs[i] for i in subset[test]]\n",
    "\n",
    "# Extract the list of tokens (words) in the dictionary\n",
    "tokens = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regresion model\n",
    "Now train the logistic regression model. The comments summarize the relevant variables created by the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train     contains information about the words within the training\n",
    "#             messages. the ith row represents the ith training message. \n",
    "#             for a particular text, the entry in the jth column tells\n",
    "#             you how many times the jth dictionary word appears in \n",
    "#             that message\n",
    "#\n",
    "# X_test      similar but for test set\n",
    "#\n",
    "# y_train     ith entry indicates whether message i is spam\n",
    "#\n",
    "# y_test      similar\n",
    "#\n",
    "\n",
    "m, n = X_train.shape\n",
    "\n",
    "theta = np.zeros(n)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: \n",
    "#  - learn theta by gradient descent \n",
    "#  - plot the cost history\n",
    "#  - tune step size and # iterations if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on test set\n",
    "Use the model fit in the previous cell to make predictions on the test set and compute the accuracy (percentage of messages in the test set that are classified correctly). You should be able to get accuracy above 95%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test, n_test = X_test.shape\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#  - use theta to make predictions for test set\n",
    "#  - print the accuracy on the test set---i.e., the precent of messages classified correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model parameters\n",
    "Run this code to examine the model parameters you just learned. These parameters assign a postive or negative value to each word --- where positive values are words that tend to be spam and negative values are words that tend to be ham. Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 spam words\n",
      "  +0.0000  zed\n",
      "  +0.0000  fathima\n",
      "  +0.0000  february\n",
      "  +0.0000  feb\n",
      "  +0.0000  fear\n",
      "  +0.0000  fb\n",
      "  +0.0000  favourite\n",
      "  +0.0000  favour\n",
      "  +0.0000  favor\n",
      "  +0.0000  fave\n",
      "\n",
      "Top 10 ham words\n",
      "  +0.0000  00\n",
      "  +0.0000  pod\n",
      "  +0.0000  point\n",
      "  +0.0000  points\n",
      "  +0.0000  police\n",
      "  +0.0000  poly\n",
      "  +0.0000  polyph\n",
      "  +0.0000  polyphonic\n",
      "  +0.0000  polys\n",
      "  +0.0000  pongal\n"
     ]
    }
   ],
   "source": [
    "token_weights = theta[1:]\n",
    "\n",
    "def reverse(a):\n",
    "    return a[::-1]\n",
    "\n",
    "most_negative = np.argsort(token_weights)\n",
    "most_positive = reverse(most_negative)\n",
    "\n",
    "k = 10\n",
    "\n",
    "print('Top %d spam words' % k)\n",
    "for i in most_positive[0:k]:\n",
    "    print('  %+.4f  %s' % (token_weights[i], tokens[i]))\n",
    "\n",
    "print('\\nTop %d ham words' % k)\n",
    "for i in most_negative[0:k]:\n",
    "    print('  %+.4f  %s' % (token_weights[i], tokens[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction on new messages\n",
    "Type a few of your own messages in below and make predictions. Are they ham or spam? Do the predictions make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(msg):\n",
    "    x = vectorizer.transform([msg]).toarray()\n",
    "    x = np.insert(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "msg = u'Write your own text here...'\n",
    "x = extract_features(msg)  # this is the feature vector\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#  - try a few texts of your own\n",
    "#  - predict whether they are spam or non-spam"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
