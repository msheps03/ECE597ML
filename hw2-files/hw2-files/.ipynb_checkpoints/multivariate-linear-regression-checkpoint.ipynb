{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Description\n",
    "\n",
    "This notebook will guide you through implementation of **multivariate linear regression** to to solve the **polynomial regression** problem:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 +  \\theta_3 x^3 + \\theta_4 x^4\n",
    "= \\boldsymbol{\\theta}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\theta_3 \\\\ \\theta_4\\end{bmatrix}, \n",
    "\\qquad\n",
    "\\mathbf{x} = \\begin{bmatrix}1 \\\\ x \\\\ x^2 \\\\ x^3 \\\\ x^4\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Below, you will\n",
    "\n",
    "1. Implement the cost function for multivarate linear regression\n",
    "1. Implement the normal equations method to solve a multivariate linear regression problem\n",
    "1. Implement gradient descent for multivariate linear regression\n",
    "1. Experiment with feature normalization to improve the convergence of gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Run this code to set up the helper functions. The function ``feature_expansion`` accepts an vector of $n$ scalar x values and returns an $n \\times 5$ data matrix by applying the feature expansion $x \\mapsto [1, x, x^2, x^3, x^4]$ to each scalar $x$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_expansion(x, deg):\n",
    "    if x.ndim > 1:\n",
    "        raise ValueError('x should be a 1-dimensional array')\n",
    "    m = x.shape\n",
    "    x_powers = [x**k for k in range(0,deg+1)]\n",
    "    X = np.stack( x_powers, axis=1 )\n",
    "\n",
    "    return X\n",
    "\n",
    "def plot_model(X_test, theta):\n",
    "    '''\n",
    "    Note: uses globals x, y, x_test, which are assigned below\n",
    "    when the dataset is created. Don't overwrite these variables.\n",
    "    '''\n",
    "    y_test = np.dot(X_test, theta)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x_test, y_test)\n",
    "    plt.legend(['Test', 'Train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) List comprehensions\n",
    "\n",
    "Read about list comprehensions. Explain what is happening in the line of code\n",
    "\n",
    "```python\n",
    "x_powers = [x**k for k in range(0,deg+1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Your answer here* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data set for polynomial regression\n",
    "\n",
    "Read and run the code below. This generates data from a fourth-degree polynomial and then uses feature expansion to set up the problem of learning the polynomial as multivariate linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create random set of m training x values between -5 and 5\n",
    "m = 100\n",
    "x = np.random.rand(m)*10 - 5   \n",
    "\n",
    "# Create evenly spaced test x values (for plotting)\n",
    "x_test  = np.linspace(-5, 5, 100)\n",
    "m_test  = len(x_test);\n",
    "\n",
    "# Feature expansion for training and test x values\n",
    "deg = 4\n",
    "X      = feature_expansion(x, deg)\n",
    "X_test = feature_expansion(x_test, deg)\n",
    "\n",
    "n = deg + 1   # total number of features including the '1' feature\n",
    "\n",
    "# Define parameters (theta) and generate y values\n",
    "theta = 0.1*np.array([1, 1, 10, 0.5, -0.5]);\n",
    "y = np.dot(X, theta) + np.random.randn(m)   # polynomial plus noise\n",
    "\n",
    "# Plot the training data\n",
    "plt.scatter(x, y)\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Implement the cost function\n",
    "Complete the code below to implement the cost function for multivariate linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, theta):  \n",
    "    '''\n",
    "    Compute the cost function for a particular data set and \n",
    "    hypothesis (parameter vector)\n",
    "    \n",
    "    Inputs: \n",
    "        X       m x n data matrix\n",
    "        y       training output (length m vector)\n",
    "        theta   parameters (length n vector)\n",
    "    Output:\n",
    "        cost    the value of the cost function (scalar)\n",
    "    '''\n",
    "    \n",
    "    # TODO: write correct code to compute the cost function\n",
    "    cost = 0\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the cost function\n",
    "Run this to test your cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "theta_random = np.random.rand(n)\n",
    "theta_zeros  = np.zeros(n)\n",
    "theta_ones   = np.ones(n)\n",
    "\n",
    "print( \"Cost function (random): %.2f\" % cost_function(X, y, theta_random))  # prints 54523.64\n",
    "print( \"Cost function  (zeros): %.2f\" % cost_function(X, y, theta_zeros))   # prints 845.65\n",
    "print( \"Cost function   (ones): %.2f\" % cost_function(X, y, theta_ones))    # prints 2524681.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6 points) Implement first training algorithm: normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equations(X, y):\n",
    "    '''\n",
    "    Train a linear regression model using the normal equations\n",
    "\n",
    "    Inputs: \n",
    "        X       m x n data matrix\n",
    "        y       training output (length m vector)\n",
    "    Output:\n",
    "        theta   parameters (length n vector)\n",
    "\n",
    "    '''\n",
    "    # TODO: write correct code to find theta using the normal equations\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use normal equations to fit the model\n",
    "Run this code to test your implementation of the normal equations. If it runs properly you will see a curve that fits the data well. Note the value of the cost function for ``theta_normal_equations``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_normal_equations = normal_equations(X, y)\n",
    "plot_model(X_test, theta_normal_equations)\n",
    "print (\"Cost function: %.2f\" % cost_function(X, y, theta_normal_equations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6 points) Implement second training algorithm: (vectorized) gradient descent\n",
    "\n",
    "Implement gradient descent for multivariate linear regression. Make sure your solution is vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent( X, y, alpha, iters, theta=None ):\n",
    "    '''\n",
    "    Train a linear regression model by gradient descent\n",
    "\n",
    "    Inputs: \n",
    "        X       m x n data matrix\n",
    "        y       training output (length m vector)\n",
    "        alpha   step size\n",
    "        iters   number of iterations\n",
    "        theta   initial parameter values (length n vector; optional)\n",
    "    \n",
    "    Output:\n",
    "        theta      learned parameters (length n vector)\n",
    "        J_history  trace of cost function value in each iteration\n",
    "\n",
    "    '''\n",
    "\n",
    "    m,n = X.shape\n",
    "    \n",
    "    if theta is None:\n",
    "        theta = np.zeros(n)\n",
    "    \n",
    "    # For recording cost function value during gradient descent\n",
    "    J_history = np.zeros(iters)\n",
    "\n",
    "    for i in range(0, iters):\n",
    "        \n",
    "        # TODO: compute gradient (vectorized) and update theta\n",
    "        \n",
    "        # Record cost function\n",
    "        J_history[i] = cost_function(X, y, theta)\n",
    "        \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4 points) Use gradient descent to train the model\n",
    "* Write code to call your ``gradient_descent`` method to learn parameter\n",
    "* Plot the model fit (use ``plot_model``)\n",
    "* Plot the cost function vs. iteration to help assess convergence\n",
    "* Print the final value of the cost function\n",
    "* Experiment with different step sizes and numbers of iterations until you can find a good hypothesis. Try to match the cost function value from ``normal_equations`` to two decimal places. How many iterations does this take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (10 points) Gradient descent with feature normalization\n",
    "You should have observed\n",
    "that it takes many iterations of gradient descent to match the cost\n",
    "function value achieved by the normal equations. Now\n",
    "you will implement feature normalization to improve the convergence\n",
    "of gradient descent. Remember that the formula\n",
    "for feature normalization is:\n",
    "\n",
    "$$x^{(i)}_j \\leftarrow \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "Here are some guidelines for the implementation:\n",
    "\n",
    "* The same transformation should be applied to train and test data.\n",
    "\n",
    "* The values $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation of\n",
    "the $j$th column (i.e., feature) in the **training data**. (Hint:\n",
    "there are numpy functions to compute these.)\n",
    "\n",
    "* Do not normalize the column of all ones. (Optional question: why?)\n",
    "\n",
    "* Use broadcasting to do the normalization--don't write for loops\n",
    "\n",
    "After normalizing both the training data and test data, follow the same steps as above to experiment with gradient descent using the *normalized* training and test data: print the value of the cost function, and create the same plots. Tune the step size and number of iterations again to make gradient descent converge as quickly as possible. How many iterations does it take to match the cost function value from ``normal_equations`` to two decimal places?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code for gradient descent with feature normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Write answer here: how many iterations?* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "Put solutions to extra credit problems here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
